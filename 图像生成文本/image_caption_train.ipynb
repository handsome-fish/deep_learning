{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Data generator\\n    a. Loads vocab\\n    b. Loads image features\\n    c. provide data for training.\\n2. Builds image caption model.\\n3. Trains the model.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. Data generator\n",
    "    a. Loads vocab\n",
    "    b. Loads image features\n",
    "    c. provide data for training.\n",
    "2. Builds image caption model.\n",
    "3. Trains the model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "import pprint\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_description_file = './dataset/flickr30k/results_20130124.token'\n",
    "input_img_feature_dir = './dataset/inception_v3_features_bakup/'\n",
    "input_vocab_file = './dataset/flickr30k/vocab.txt'\n",
    "output_dir = './local_run/'\n",
    "\n",
    "if not gfile.Exists(output_dir):\n",
    "    gfile.MakeDirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_vocab_word_threshold = 3,\n",
    "        num_embedding_size = 32,\n",
    "        num_timesteps = 10,\n",
    "        num_lstm_nodes = [64, 64],\n",
    "        num_lstm_layers = 2,\n",
    "        num_fc_nodes = 32,\n",
    "        batch_size = 80,\n",
    "        cell_type = 'lstm',\n",
    "        clip_lstm_grads = 1.0,\n",
    "        learning_rate = 0.001,\n",
    "        keep_prob = 0.8,\n",
    "        log_frequent = 10,\n",
    "        save_frequent = 100\n",
    "    )\n",
    "\n",
    "hps = get_default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 10875\n",
      "[1494, 4709, 1, 0, 2, 0]\n",
      "'in the snow .'\n"
     ]
    }
   ],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, filename, num_vocab_word_threshold):\n",
    "        self._id_to_word = {}\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._num_vocab_word_threshold = num_vocab_word_threshold\n",
    "        self._read_dict(filename)\n",
    "        \n",
    "    def _read_dict(self, filename):\n",
    "        with gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\n\\r').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_vocab_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "#             pprint.pprint(self._word_to_id.items())\n",
    "#             if word in self._id_to_word or self._word_to_id:\n",
    "#                 raise Exception(\"duplicate words in vocab. %s\" % word)\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "            \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "    \n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self.unk)\n",
    "    \n",
    "    def id_to_word(self, idx):\n",
    "        return self._id_to_word.get(idx, '<UNK>')\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        return [self.word_to_id(word) for word in sentence.strip('\\r\\t').split(' ')]\n",
    "    \n",
    "    def decode(self, sentence_id):\n",
    "        words = [self.id_to_word(idx) for idx in sentence_id]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "vocab = Vocab(input_vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "\n",
    "logging.info(\"vocab_size: %d\", vocab_size)\n",
    "pprint.pprint(vocab.encode(\"I am a dreamer . \"))\n",
    "pprint.pprint(vocab.decode([4, 5, 100, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:num of all images: 31783\n",
      "['A man in jeans is reclining on a green metal bench along a busy sidewalk and '\n",
      " 'crowded street .',\n",
      " 'A white male with a blue sweater and gray pants laying on a sidewalk bench .',\n",
      " 'A man in a blue shirt and gray pants is sleeping on a sidewalk bench .',\n",
      " 'A person is sleeping on a bench , next to cars .',\n",
      " 'A man sleeping on a bench in a city area .']\n",
      "INFO:tensorflow:num of all images: 31783\n",
      "[[3, 9, 4, 132, 8, 3532, 6, 1, 48, 337, 146, 139, 1, 244, 93, 7, 380, 36, 2],\n",
      " [3, 20, 179, 11, 1, 26, 284, 7, 120, 128, 297, 6, 1, 93, 146, 2],\n",
      " [3, 9, 4, 1, 26, 21, 7, 120, 128, 8, 340, 6, 1, 93, 146, 2],\n",
      " [3, 63, 8, 340, 6, 1, 146, 12, 70, 15, 518, 2],\n",
      " [3, 9, 340, 6, 1, 146, 4, 1, 112, 171, 2]]\n"
     ]
    }
   ],
   "source": [
    "def parse_token_file(token_file):\n",
    "    '''Parses input_description_file to a dict'''\n",
    "    img_name_to_token = {}\n",
    "    with gfile.GFile(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    for line in lines:\n",
    "        img_id, description = line.strip('\\n\\r').split('\\t')\n",
    "        img_name = img_id.split('#')[0]\n",
    "        img_name_to_token.setdefault(img_name, [])\n",
    "        img_name_to_token[img_name].append(description)\n",
    "        \n",
    "    return img_name_to_token\n",
    "\n",
    "def convert_token_to_id(img_name_to_token, vocab):\n",
    "    '''Converts tokens of each description of img to id'''\n",
    "    img_name_to_token_id = {}\n",
    "    for img_name in img_name_to_token:\n",
    "        img_name_to_token_id.setdefault(img_name, [])\n",
    "        for description in img_name_to_token[img_name]:\n",
    "            tokens_ids = vocab.encode(description)\n",
    "            img_name_to_token_id[img_name].append(tokens_ids)\n",
    "            \n",
    "    return img_name_to_token_id\n",
    "\n",
    "img_name_to_token = parse_token_file(input_description_file)\n",
    "img_name_to_token_id = convert_token_to_id(img_name_to_token, vocab)\n",
    "\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_token))\n",
    "pprint.pprint(img_name_to_token['2778832101.jpg'])\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_token_id))\n",
    "pprint.pprint(img_name_to_token_id['2778832101.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./dataset/inception_v3_features_bakup/image_feature-0.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-1.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-10.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-11.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-12.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-13.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-14.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-15.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-16.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-17.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-18.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-19.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-2.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-20.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-21.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-22.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-23.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-24.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-25.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-26.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-27.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-28.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-29.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-3.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-30.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-31.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-4.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-5.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-6.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-7.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-8.pickle',\n",
      " './dataset/inception_v3_features_bakup/image_feature-9.pickle']\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-0.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-1.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-10.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-11.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-12.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-13.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-14.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-15.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-16.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-17.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-18.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-19.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-2.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-20.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-21.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-22.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-23.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-24.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-25.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-26.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-27.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-28.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-29.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-3.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-30.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-31.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-4.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-5.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-6.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-7.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-8.pickle\n",
      "INFO:tensorflow:loading ./dataset/inception_v3_features_bakup/image_feature-9.pickle\n",
      "(31783, 2048)\n",
      "(31783,)\n",
      "INFO:tensorflow:img_feature_dim: 2048\n",
      "INFO:tensorflow:caption_data_size: 31783\n",
      "array([[0.28064385, 0.08373295, 0.23600066, ..., 0.10185016, 0.3908017 ,\n",
      "        0.13824494],\n",
      "       [0.18090765, 0.12749995, 0.3925953 , ..., 0.16350196, 0.32915944,\n",
      "        0.1428829 ],\n",
      "       [0.07302605, 0.3808588 , 0.15557338, ..., 0.27417555, 0.43747833,\n",
      "        0.01422254],\n",
      "       [0.32355705, 0.44524324, 0.4079168 , ..., 0.28012997, 0.20847787,\n",
      "        0.64114356],\n",
      "       [0.15255915, 0.01820992, 0.06999099, ..., 0.7032823 , 1.1322318 ,\n",
      "        0.19903131]], dtype=float32)\n",
      "array([[   3,   88, 1595,   82,   27,    0, 1262,   18,    1,   20],\n",
      "       [  47,   23,   69,   80,    4,    1,  225,  144,   17,    1],\n",
      "       [ 452,   23,   34,    4,   20,  322, 1286,    7,  241,  123],\n",
      "       [   3,    9,    4,    1,  191,    7,  620,    8,  987,    1],\n",
      "       [   3,   22,   33,   35,    4,    1,  312,    2,    2,    2]])\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "array(['4691424506.jpg', '1787222774.jpg', '4946986337.jpg',\n",
      "       '3732782360.jpg', '1465666502.jpg'], dtype='<U14')\n"
     ]
    }
   ],
   "source": [
    "class ImageCaptionData(object):\n",
    "    '''Provides data for image caption model.'''\n",
    "    def __init__(self, \n",
    "                 img_name_to_token_id, \n",
    "                 img_feature_dir,\n",
    "                 num_timesteps,\n",
    "                 vocab,\n",
    "                 deterministic = False):\n",
    "        self._vocab = vocab\n",
    "        self._img_name_to_token_id = img_name_to_token_id\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._deterministic = deterministic\n",
    "        self._indicator = 0\n",
    "        \n",
    "        self._img_feature_filenames = []\n",
    "        self._img_feature_data = []\n",
    "        \n",
    "        self._all_img_feature_filepaths = []\n",
    "        for filename in gfile.ListDirectory(img_feature_dir):\n",
    "            self._all_img_feature_filepaths.append(\n",
    "                os.path.join(img_feature_dir, filename))\n",
    "        pprint.pprint(self._all_img_feature_filepaths)\n",
    "        self._load_img_feature_pickle()\n",
    "        \n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "            \n",
    "    def _load_img_feature_pickle(self):\n",
    "        '''Load img feature data from pickle.'''\n",
    "        for filepath in self._all_img_feature_filepaths:\n",
    "            logging.info(\"loading %s\" % filepath)\n",
    "            with gfile.GFile(filepath, 'rb') as f:\n",
    "                filenames, features = pickle.load(f)\n",
    "                self._img_feature_filenames += filenames\n",
    "                self._img_feature_data.append(features)\n",
    "        # [#(1000, 1, 1, 2048), #(1000, 1, 1, 2048)] -> #(2000, 1, 1, 2048)\n",
    "        self._img_feature_data = np.vstack(self._img_feature_data)\n",
    "        origin_shape = self._img_feature_data.shape\n",
    "        self._img_feature_data = np.reshape(\n",
    "            self._img_feature_data,\n",
    "            (origin_shape[0], origin_shape[-1]))\n",
    "        self._img_feature_filenames = np.asarray(self._img_feature_filenames)\n",
    "        print(self._img_feature_data.shape)\n",
    "        print(self._img_feature_filenames.shape)\n",
    "        \n",
    "    def size(self):\n",
    "        return len(self._img_feature_filenames)\n",
    "    \n",
    "    def img_feature_size(self):\n",
    "        return self._img_feature_data.shape[1]\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        '''Shuffle data randomly.'''\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._img_feature_filenames = self._img_feature_filenames[p]\n",
    "        self._img_feature_data = self._img_feature_data[p]\n",
    "        \n",
    "    def _img_desc(self, batch_filenames):\n",
    "        '''Gets description for filenames in batch.'''\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        for filename in batch_filenames:\n",
    "            token_ids_set = self._img_name_to_token_id[filename]\n",
    "            chosen_token_ids = random.choice(token_ids_set)\n",
    "            chosen_token_ids_length = len(chosen_token_ids)\n",
    "            \n",
    "            weight = [1 for i in range(chosen_token_ids_length)]\n",
    "            if chosen_token_ids_length >= self._num_timesteps:\n",
    "                chosen_token_ids = chosen_token_ids[0: self._num_timesteps]\n",
    "                weight = weight[0: self._num_timesteps]\n",
    "            else:\n",
    "                remaining_length = self._num_timesteps - chosen_token_ids_length\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)]\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        '''Returns next batch data.'''\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.size():\n",
    "            if not self._deterministic:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator < self.size()\n",
    "        \n",
    "        batch_filenames = self._img_feature_filenames[self._indicator: end_indicator]\n",
    "        batch_img_features = self._img_feature_data[self._indicator: end_indicator]\n",
    "        # sentence_ids: [100, 101, 102, 10, 0, 3, 2, 0, 0] -> [1, 1, 1, 1, 0, 1, 1, 0, 0]\n",
    "        # id为0的weight为0，其他为1\n",
    "        batch_sentence_ids, batch_weights = self._img_desc(batch_filenames)\n",
    "        self._indicator = end_indicator\n",
    "        return batch_img_features, batch_sentence_ids, batch_weights, batch_filenames\n",
    "    \n",
    "caption_data = ImageCaptionData(img_name_to_token_id, \n",
    "                                input_img_feature_dir,\n",
    "                                hps.num_timesteps,\n",
    "                                vocab)\n",
    "img_feature_dim = caption_data.img_feature_size()\n",
    "caption_data_size = caption_data.size()\n",
    "logging.info(\"img_feature_dim: %d\" % img_feature_dim)\n",
    "logging.info(\"caption_data_size: %d\" % caption_data_size)\n",
    "\n",
    "batch_img_features, batch_sentence_ids, batch_weights, batch_img_names = caption_data.next_batch(5)\n",
    "pprint.pprint(batch_img_features)\n",
    "pprint.pprint(batch_sentence_ids)\n",
    "pprint.pprint(batch_weights)\n",
    "pprint.pprint(batch_img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-56e1d536f6fc>:43: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-8-56e1d536f6fc>:5: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "INFO:tensorflow:variable name: <tf.Variable 'embedding/embeddings:0' shape=(10875, 32) dtype=float32_ref>\n",
      "INFO:tensorflow:variable name: <tf.Variable 'img_feature_embed/dense/kernel:0' shape=(2048, 32) dtype=float32_ref>\n",
      "INFO:tensorflow:variable name: <tf.Variable 'img_feature_embed/dense/bias:0' shape=(32,) dtype=float32_ref>\n",
      "INFO:tensorflow:variable name: <tf.Variable 'lstm_rnn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(96, 256) dtype=float32_ref>\n",
      "INFO:tensorflow:variable name: <tf.Variable 'lstm_rnn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(256,) dtype=float32_ref>\n",
      "INFO:tensorflow:variable name: <tf.Variable 'lstm_rnn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(128, 256) dtype=float32_ref>\n",
      "INFO:tensorflow:variable name: <tf.Variable 'lstm_rnn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(256,) dtype=float32_ref>\n",
      "INFO:tensorflow:variable name: <tf.Variable 'fc/fc1/kernel:0' shape=(64, 32) dtype=float32_ref>\n",
      "INFO:tensorflow:variable name: <tf.Variable 'fc/fc1/bias:0' shape=(32,) dtype=float32_ref>\n",
      "INFO:tensorflow:variable name: <tf.Variable 'fc/logits/kernel:0' shape=(32, 10875) dtype=float32_ref>\n",
      "INFO:tensorflow:variable name: <tf.Variable 'fc/logits/bias:0' shape=(10875,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    '''Returns specific cell according to cell_type.'''\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim, \n",
    "                                            state_is_tuple = True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s type has not been supported.\" % cell_type)\n",
    "\n",
    "def dropout(cell, keep_prob):\n",
    "    '''Wrap cell with dropout.'''\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob = keep_prob)\n",
    "\n",
    "def get_train_model(hps, vocab_size, img_feature_dim):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    img_feature = tf.placeholder(tf.float32, (batch_size, img_feature_dim))\n",
    "    sentence = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    mask = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    global_step = tf.Variable(tf.zeros([], tf.int32),\n",
    "                              name = \"global_step\",\n",
    "                              trainable = False)\n",
    "    \n",
    "    # prediction process:\n",
    "    # sentence: [a, b, c, d, e]\n",
    "    # input: [img, a, b, c, d]\n",
    "    # img_feature: [0.4, 0.3, 10, 2]\n",
    "    # prediction1: img_feature -> embedding_img -> lstm -> (a)\n",
    "    # prediction2: a -> embedding_word -> lstm -> (b)\n",
    "    # prediction3: b -> embedding_word -> lstm -> (c)\n",
    "    # ...\n",
    "    \n",
    "    # Sets up embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer = embedding_initializer):\n",
    "        embeddings = tf.get_variable('embeddings', [vocab_size, hps.num_embedding_size], tf.float32)\n",
    "        # embed_token_ids: [batch_size, num_timesteps - 1, num_embedding_size]\n",
    "        embed_token_ids = tf.nn.embedding_lookup(embeddings, sentence[:, 0:num_timesteps - 1])\n",
    "    \n",
    "    img_feature_embed_init = tf.uniform_unit_scaling_initializer(factor = 1.0)\n",
    "    with tf.variable_scope('img_feature_embed', initializer = img_feature_embed_init):\n",
    "        # img_feature: [batch_size, img_feature_dim]\n",
    "        # embed_img: [batch_size, num_embedding_size]\n",
    "        embed_img = tf.layers.dense(img_feature, hps.num_embedding_size)\n",
    "        # embed_img: [batch_size, 1, num_embedding_size]\n",
    "        embed_img = tf.expand_dims(embed_img, 1)\n",
    "        # embed_inputs: [batch_size, num_timesteps, num_embedding_size]\n",
    "        embed_inputs = tf.concat([embed_img, embed_token_ids], axis = 1)\n",
    "    \n",
    "    # Sets up rnn network\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    rnn_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_rnn', initializer = rnn_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        \n",
    "        init_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        # run_outputs: [batch_size, num_timesteps, hps.num_lstm_nodes[-1]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(cell, \n",
    "                                           embed_inputs,\n",
    "                                           initial_state = init_state)\n",
    "        \n",
    "    # Sets up fully-connected layer.\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor = 1.0)\n",
    "    with tf.variable_scope('fc', initializer = fc_init):\n",
    "        rnn_outputs_2d = tf.reshape(rnn_outputs, \n",
    "                                   [-1, hps.num_lstm_nodes[-1]])\n",
    "        fc1 = tf.layers.dense(rnn_outputs_2d,\n",
    "                              hps.num_fc_nodes, \n",
    "                              name = 'fc1')\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        fc1_relu = tf.nn.relu(fc1_dropout)\n",
    "        \n",
    "        logits = tf.layers.dense(fc1_relu, \n",
    "                                 vocab_size, \n",
    "                                 name = 'logits')\n",
    "        \n",
    "    # Calculates loss\n",
    "    with tf.variable_scope('loss'):\n",
    "        sentence_flatten = tf.reshape(sentence, [-1])\n",
    "        mask_flatten = tf.reshape(mask, [-1])\n",
    "        mask_flatten_float = tf.cast(mask_flatten, tf.float32)\n",
    "        mask_sum = tf.reduce_sum(mask_flatten)\n",
    "        mask_sum_float = tf.cast(mask_sum, tf.float32)\n",
    "        \n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits = logits,\n",
    "            labels = sentence_flatten)\n",
    "        \n",
    "        weighted_softmax_loss = tf.multiply(\n",
    "            softmax_loss, mask_flatten_float)\n",
    "        loss = tf.reduce_sum(weighted_softmax_loss) / mask_sum_float\n",
    "        \n",
    "        prediction = tf.argmax(logits, 1, output_type = tf.int32)\n",
    "        correct_prediction = tf.equal(prediction, sentence_flatten)\n",
    "        weighted_correct_prediction = tf.multiply(\n",
    "            tf.cast(correct_prediction, tf.float32), mask_flatten_float)\n",
    "        accuracy = tf.reduce_sum(weighted_correct_prediction) / mask_sum_float\n",
    "        tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    # Defines train_op.\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            logging.info(\"variable name: %s\", var.name)\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars), global_step = global_step)\n",
    "        \n",
    "    return ((img_feature, sentence, mask, keep_prob),\n",
    "            (loss, accuracy, train_op),\n",
    "            global_step)\n",
    "\n",
    "placeholders, metrics, global_step = get_train_model(hps, vocab_size, img_feature_dim)\n",
    "img_feature, sentence, mask, keep_prob = placeholders\n",
    "loss, accuracy, train_op = metrics\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:    10, loss: 9.245, accuracy: 0.000\n",
      "INFO:tensorflow:Step:    20, loss: 8.952, accuracy: 0.000\n",
      "INFO:tensorflow:Step:    30, loss: 8.704, accuracy: 0.015\n",
      "INFO:tensorflow:Step:    40, loss: 8.297, accuracy: 0.029\n",
      "INFO:tensorflow:Step:    50, loss: 7.675, accuracy: 0.055\n",
      "INFO:tensorflow:Step:    60, loss: 7.159, accuracy: 0.075\n",
      "INFO:tensorflow:Step:    70, loss: 6.418, accuracy: 0.114\n",
      "INFO:tensorflow:Step:    80, loss: 6.244, accuracy: 0.129\n",
      "INFO:tensorflow:Step:    90, loss: 5.916, accuracy: 0.143\n",
      "INFO:tensorflow:Step:   100, loss: 5.982, accuracy: 0.130\n",
      "INFO:tensorflow:Step:   100, model saved\n",
      "INFO:tensorflow:Step:   110, loss: 5.912, accuracy: 0.130\n",
      "INFO:tensorflow:Step:   120, loss: 5.845, accuracy: 0.151\n",
      "INFO:tensorflow:Step:   130, loss: 5.567, accuracy: 0.155\n",
      "INFO:tensorflow:Step:   140, loss: 5.730, accuracy: 0.133\n",
      "INFO:tensorflow:Step:   150, loss: 5.777, accuracy: 0.117\n",
      "INFO:tensorflow:Step:   160, loss: 5.790, accuracy: 0.123\n",
      "INFO:tensorflow:Step:   170, loss: 5.433, accuracy: 0.167\n",
      "INFO:tensorflow:Step:   180, loss: 5.746, accuracy: 0.155\n",
      "INFO:tensorflow:Step:   190, loss: 5.475, accuracy: 0.146\n",
      "INFO:tensorflow:Step:   200, loss: 5.392, accuracy: 0.183\n",
      "INFO:tensorflow:Step:   200, model saved\n",
      "INFO:tensorflow:Step:   210, loss: 5.456, accuracy: 0.164\n",
      "INFO:tensorflow:Step:   220, loss: 5.588, accuracy: 0.172\n",
      "INFO:tensorflow:Step:   230, loss: 5.423, accuracy: 0.154\n",
      "INFO:tensorflow:Step:   240, loss: 5.425, accuracy: 0.139\n",
      "INFO:tensorflow:Step:   250, loss: 5.704, accuracy: 0.136\n",
      "INFO:tensorflow:Step:   260, loss: 5.494, accuracy: 0.145\n",
      "INFO:tensorflow:Step:   270, loss: 5.327, accuracy: 0.171\n",
      "INFO:tensorflow:Step:   280, loss: 5.325, accuracy: 0.170\n",
      "INFO:tensorflow:Step:   290, loss: 5.187, accuracy: 0.178\n",
      "INFO:tensorflow:Step:   300, loss: 5.435, accuracy: 0.150\n",
      "INFO:tensorflow:Step:   300, model saved\n",
      "INFO:tensorflow:Step:   310, loss: 5.362, accuracy: 0.192\n",
      "INFO:tensorflow:Step:   320, loss: 5.259, accuracy: 0.177\n",
      "INFO:tensorflow:Step:   330, loss: 5.401, accuracy: 0.173\n",
      "INFO:tensorflow:Step:   340, loss: 5.237, accuracy: 0.181\n",
      "INFO:tensorflow:Step:   350, loss: 5.494, accuracy: 0.169\n",
      "INFO:tensorflow:Step:   360, loss: 5.219, accuracy: 0.164\n",
      "INFO:tensorflow:Step:   370, loss: 5.383, accuracy: 0.175\n",
      "INFO:tensorflow:Step:   380, loss: 5.176, accuracy: 0.177\n",
      "INFO:tensorflow:Step:   390, loss: 5.443, accuracy: 0.168\n",
      "INFO:tensorflow:Step:   400, loss: 4.968, accuracy: 0.201\n",
      "INFO:tensorflow:Step:   400, model saved\n",
      "INFO:tensorflow:Step:   410, loss: 5.233, accuracy: 0.192\n",
      "INFO:tensorflow:Step:   420, loss: 5.083, accuracy: 0.205\n",
      "INFO:tensorflow:Step:   430, loss: 5.039, accuracy: 0.208\n",
      "INFO:tensorflow:Step:   440, loss: 5.107, accuracy: 0.203\n",
      "INFO:tensorflow:Step:   450, loss: 4.935, accuracy: 0.215\n",
      "INFO:tensorflow:Step:   460, loss: 5.131, accuracy: 0.188\n",
      "INFO:tensorflow:Step:   470, loss: 5.009, accuracy: 0.222\n",
      "INFO:tensorflow:Step:   480, loss: 5.106, accuracy: 0.217\n",
      "INFO:tensorflow:Step:   490, loss: 4.930, accuracy: 0.218\n",
      "INFO:tensorflow:Step:   500, loss: 5.089, accuracy: 0.190\n",
      "INFO:tensorflow:Step:   500, model saved\n",
      "INFO:tensorflow:Step:   510, loss: 5.159, accuracy: 0.215\n",
      "INFO:tensorflow:Step:   520, loss: 4.981, accuracy: 0.197\n",
      "INFO:tensorflow:Step:   530, loss: 4.820, accuracy: 0.234\n",
      "INFO:tensorflow:Step:   540, loss: 4.924, accuracy: 0.204\n",
      "INFO:tensorflow:Step:   550, loss: 4.990, accuracy: 0.224\n",
      "INFO:tensorflow:Step:   560, loss: 4.718, accuracy: 0.251\n",
      "INFO:tensorflow:Step:   570, loss: 4.863, accuracy: 0.228\n",
      "INFO:tensorflow:Step:   580, loss: 4.593, accuracy: 0.277\n",
      "INFO:tensorflow:Step:   590, loss: 4.914, accuracy: 0.225\n",
      "INFO:tensorflow:Step:   600, loss: 4.710, accuracy: 0.251\n",
      "INFO:tensorflow:Step:   600, model saved\n",
      "INFO:tensorflow:Step:   610, loss: 4.886, accuracy: 0.216\n",
      "INFO:tensorflow:Step:   620, loss: 4.851, accuracy: 0.232\n",
      "INFO:tensorflow:Step:   630, loss: 4.477, accuracy: 0.270\n",
      "INFO:tensorflow:Step:   640, loss: 4.672, accuracy: 0.260\n",
      "INFO:tensorflow:Step:   650, loss: 4.904, accuracy: 0.238\n",
      "INFO:tensorflow:Step:   660, loss: 4.589, accuracy: 0.254\n",
      "INFO:tensorflow:Step:   670, loss: 4.996, accuracy: 0.236\n",
      "INFO:tensorflow:Step:   680, loss: 4.755, accuracy: 0.239\n",
      "INFO:tensorflow:Step:   690, loss: 4.946, accuracy: 0.227\n",
      "INFO:tensorflow:Step:   700, loss: 4.798, accuracy: 0.243\n",
      "INFO:tensorflow:Step:   700, model saved\n",
      "INFO:tensorflow:Step:   710, loss: 4.793, accuracy: 0.253\n",
      "INFO:tensorflow:Step:   720, loss: 4.744, accuracy: 0.205\n",
      "INFO:tensorflow:Step:   730, loss: 4.794, accuracy: 0.232\n",
      "INFO:tensorflow:Step:   740, loss: 4.465, accuracy: 0.253\n",
      "INFO:tensorflow:Step:   750, loss: 4.723, accuracy: 0.252\n",
      "INFO:tensorflow:Step:   760, loss: 4.796, accuracy: 0.226\n",
      "INFO:tensorflow:Step:   770, loss: 4.720, accuracy: 0.270\n",
      "INFO:tensorflow:Step:   780, loss: 4.943, accuracy: 0.220\n",
      "INFO:tensorflow:Step:   790, loss: 4.665, accuracy: 0.236\n",
      "INFO:tensorflow:Step:   800, loss: 4.538, accuracy: 0.276\n",
      "INFO:tensorflow:Step:   800, model saved\n",
      "INFO:tensorflow:Step:   810, loss: 4.766, accuracy: 0.229\n",
      "INFO:tensorflow:Step:   820, loss: 4.412, accuracy: 0.254\n",
      "INFO:tensorflow:Step:   830, loss: 4.483, accuracy: 0.277\n",
      "INFO:tensorflow:Step:   840, loss: 4.592, accuracy: 0.239\n",
      "INFO:tensorflow:Step:   850, loss: 4.677, accuracy: 0.252\n",
      "INFO:tensorflow:Step:   860, loss: 4.734, accuracy: 0.254\n",
      "INFO:tensorflow:Step:   870, loss: 4.697, accuracy: 0.259\n",
      "INFO:tensorflow:Step:   880, loss: 4.669, accuracy: 0.244\n",
      "INFO:tensorflow:Step:   890, loss: 4.559, accuracy: 0.247\n",
      "INFO:tensorflow:Step:   900, loss: 4.838, accuracy: 0.236\n",
      "INFO:tensorflow:Step:   900, model saved\n",
      "INFO:tensorflow:Step:   910, loss: 4.642, accuracy: 0.273\n",
      "INFO:tensorflow:Step:   920, loss: 4.639, accuracy: 0.235\n",
      "INFO:tensorflow:Step:   930, loss: 4.670, accuracy: 0.255\n",
      "INFO:tensorflow:Step:   940, loss: 4.616, accuracy: 0.250\n",
      "INFO:tensorflow:Step:   950, loss: 4.564, accuracy: 0.268\n",
      "INFO:tensorflow:Step:   960, loss: 4.603, accuracy: 0.255\n",
      "INFO:tensorflow:Step:   970, loss: 4.533, accuracy: 0.269\n",
      "INFO:tensorflow:Step:   980, loss: 4.485, accuracy: 0.256\n",
      "INFO:tensorflow:Step:   990, loss: 4.387, accuracy: 0.272\n",
      "INFO:tensorflow:Step:  1000, loss: 4.617, accuracy: 0.254\n",
      "INFO:tensorflow:Step:  1000, model saved\n"
     ]
    }
   ],
   "source": [
    "training_steps = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    writer = tf.summary.FileWriter(output_dir, sess.graph)\n",
    "    for i in range(training_steps):\n",
    "        batch_img_features, batch_sentence_ids, batch_weights, _ = caption_data.next_batch(hps.batch_size)\n",
    "        input_vals = (batch_img_features, batch_sentence_ids, batch_weights, hps.keep_prob)\n",
    "        feed_dict = dict(zip(placeholders, input_vals))\n",
    "        fetches = [global_step, loss, accuracy, train_op]\n",
    "        should_log = (i+1) % hps.log_frequent == 0\n",
    "        should_save = (i+1) % hps.save_frequent == 0\n",
    "        \n",
    "        if should_log:\n",
    "            fetches += [summary_op]\n",
    "        \n",
    "        outputs = sess.run(fetches, feed_dict = feed_dict)\n",
    "        global_step_val, loss_val, accuracy_val = outputs[0:3]\n",
    "        if should_log:\n",
    "            summary_str = outputs[-1]\n",
    "            writer.add_summary(summary_str, global_step_val)\n",
    "            logging.info(\"Step: %5d, loss: %3.3f, accuracy: %3.3f\" % (global_step_val, loss_val, accuracy_val))\n",
    "        \n",
    "        if should_save:\n",
    "            model_save_file = os.path.join(output_dir, \"image_caption\")\n",
    "            logging.info(\"Step: %5d, model saved\" % global_step_val)\n",
    "            saver.save(sess, model_save_file, global_step = global_step_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
