{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建计算图\n",
    "#     embedding\n",
    "#     LSTM\n",
    "#     fc\n",
    "#     train_op\n",
    "# 训练流程代码\n",
    "# 数据集封装\n",
    "#     api: next_batch(batch_size)\n",
    "# 词表封装:\n",
    "#     api: sentence2id(text_sentence): 句子转换id\n",
    "# 类别封装：\n",
    "#     api:category2id(text_category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_embedding_size = 32,\n",
    "        num_timesteps = 600,\n",
    "        num_lstm_nodes = [64, 64],\n",
    "        num_lstm_layers = 2, \n",
    "        num_fc_nodes = 64,\n",
    "        batch_size = 100,\n",
    "        clip_lstm_grads = 1.0,\n",
    "        learning_rate = 0.001,\n",
    "        num_word_threshold = 10\n",
    "    )\n",
    "\n",
    "hps = get_default_params()\n",
    "\n",
    "train_file = './cnews/cnews.trian.seg.txt'\n",
    "val_file = './cnews/cnews.val.seg.txt'\n",
    "test_file = './cnews/cnews.test.seg.txt'\n",
    "vocab_file = './cnews/cnews.vocab.txt'\n",
    "category_file = './cnews/cnews.category.txt'\n",
    "\n",
    "output_file = './cnews/run_text_rnn'\n",
    "if not os.path.exists(output_file):\n",
    "    os.mkdir(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 77323\n",
      "INFO:tensorflow:label: ['的', 'stes', '在'], id: [2, 0, 4]\n",
      "INFO:tensorflow:num_classes: 10\n",
      "INFO:tensorflow:label: 时尚, id: 5\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "                       \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r', encoding = 'utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line  in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "            \n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) \\\n",
    "                    for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "    \n",
    "\n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r', encoding = 'utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            category = line.strip('\\t\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "            \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "\n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Exception(\"%s is not in our category list\" % category)\n",
    "        return self._category_to_id[category]\n",
    "    \n",
    "vocab = Vocab(vocab_file, hps.num_word_threshold)\n",
    "tf.logging.info('vocab_size: %d' % vocab.size())\n",
    "test_str = '的 stes 在'\n",
    "tf.logging.info(\"label: \" + str(list(test_str.split(\" \"))) + \", id: \" + str(vocab.sentence_to_id(test_str)))\n",
    "# print(test_str + str(vocab.sentence_to_id(test_str)))\n",
    "\n",
    "category_vocab = CategoryDict(category_file)\n",
    "tf.logging.info('num_classes: %d', category_vocab.size())\n",
    "test_category = '时尚'\n",
    "# print(category_vocab.category_to_id(test_category))\n",
    "tf.logging.info(\"label: %s, id: %d\" % (test_category, category_vocab.category_to_id(test_category)))\n",
    "\n",
    "vocab_size = vocab.size()\n",
    "num_classes = category_vocab.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading data from ./cnews/cnews.trian.seg.txt\n",
      "INFO:tensorflow:Loading data from ./cnews/cnews.val.seg.txt\n",
      "INFO:tensorflow:Loading data from ./cnews/cnews.test.seg.txt\n",
      "(array([[    0,   761,    53,    41,  8047,    17,   250,    16,    60,\n",
      "            0,   761,    53,    41,  8047,     1,   677,     7,  2032,\n",
      "            2,   947,    32, 18935, 13794,   129,  6270,     1,  7551,\n",
      "          324,    10, 15326,     3,   119,     2,     0,     2,  2664,\n",
      "          235,    81,    25,  1891,     2,   416,   384,  3120,     0,\n",
      "            0,     0,     0,     0,     0],\n",
      "       [27699,    11, 29205, 13860,  5902,     8,     0,     9, 22471,\n",
      "          160,    71,    15, 27699,    14,   270,     1,    71,     0,\n",
      "         6141,     2,  2252,     1,  1325,     4,  2088,     0,   755,\n",
      "            2,   300,    29,   244,   453,     3,   443,  2271,   751,\n",
      "        28565,  6682, 44405,   991,     1,   799,     1,   991,    52,\n",
      "          117,  4503,   129,     0,     3]]), array([2, 4]))\n",
      "(array([[  644,  1400,    15,     0,  1491,    14,  1470,  7135,  5688,\n",
      "         5700,   183,    89,     1,   258,     7,   348,     0,     3,\n",
      "          500, 21226,   165, 27427,  9282,    21,     2,  9559,   384,\n",
      "            3,    53,    25,     2,  9282,  2821,   504,     1,    26,\n",
      "           46,     2,   865,    47,    55,   306,    55,  1960,     3,\n",
      "          276,  9320,     2,    78,   571],\n",
      "       [ 3705, 29487, 38593, 12505, 29915,   340,  2414,  1550,   153,\n",
      "           23,   103,   814,   160,   298,    15, 24700,    14,   270,\n",
      "            1,  7873,  6512,   433,     2,  3705, 29487, 38593,   407,\n",
      "            1,   744, 16235,     0, 29335,   113,  1241,  3858,   298,\n",
      "            0,  1668,     1, 31253,     0,  2311,  1330,  1527,  1438,\n",
      "        15676,     3,    72,  1745,     1]]), array([7, 6]))\n",
      "(array([[  467,    11,  2947,     0,  8292,  9795,  5056,     0,  2320,\n",
      "           11,  4034,     5,  1408,     5, 10622,     1, 20920,  9107,\n",
      "            2, 33924,   603,  1001,     1,   118,  1189,  9795,     2,\n",
      "         7419,  6480,     1,  7721,  1619,    27,   125,    38,     0,\n",
      "          603,  8648,     3,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0],\n",
      "       [63289, 18267,   308,    51,    41,  1409, 20746,    11,   105,\n",
      "          676,    25,    47,   893,  2878,  3377,    76,  6200, 35248,\n",
      "          103,    23,   101,    44,   270,   952,  6967, 25873,     1,\n",
      "         4109,   823,  1421,    63,    10,  3144,  5058,     1,   585,\n",
      "         2110,  3225,    21,     0,   562,     1,    24,    73,   344,\n",
      "         2294,     3,   452,     1,    35]]), array([5, 0]))\n"
     ]
    }
   ],
   "source": [
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vacab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vacab = category_vacab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # matrix\n",
    "        self._inputs = []\n",
    "        # vector\n",
    "        self._outputs = []\n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "        \n",
    "    def _parse_file(self, filename):\n",
    "        tf.logging.info(\"Loading data from %s\" % filename)\n",
    "        with open(filename, 'r', encoding = 'utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\t\\r').split('\\t')\n",
    "            id_label = self._category_vacab.category_to_id(label)\n",
    "            id_words = self._vocab.sentence_to_id(content)\n",
    "            # 统一mini_batch\n",
    "            id_words = id_words[0: self._num_timesteps]\n",
    "            padding_num = self._num_timesteps - len(id_words)\n",
    "            id_words = id_words + [\n",
    "                self._vocab.unk for i in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "        self._inputs = np.asarray(self._inputs, dtype = np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype = np.int32)\n",
    "        self._random_shuffle()\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Exception(\"batch_size: %d is too large\" % batch_size)\n",
    "        batch_inputs = self._inputs[self._indicator:end_indicator]\n",
    "        batch_outputs = self._outputs[self._indicator:end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_outputs\n",
    "    \n",
    "train_dataset = TextDataSet(\n",
    "    train_file, vocab, category_vocab, hps.num_timesteps)\n",
    "val_dataset = TextDataSet(\n",
    "    val_file, vocab, category_vocab, hps.num_timesteps)\n",
    "test_dataset = TextDataSet(\n",
    "    test_file, vocab, category_vocab, hps.num_timesteps)\n",
    "\n",
    "print(train_dataset.next_batch(2))\n",
    "print(val_dataset.next_batch(2))\n",
    "print(test_dataset.next_batch(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-0918a34923b7>:31: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:From <ipython-input-5-0918a34923b7>:44: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "INFO:tensorflow:variable name: global_step:0\n",
      "INFO:tensorflow:variable name: embedding/embedding:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc2/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc2/bias:0\n"
     ]
    }
   ],
   "source": [
    "def create_model(hps, vocab_size, num_classes):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    outputs = tf.placeholder(tf.int32, (batch_size, ))\n",
    "    # 表示保留数，用于dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    # 训练到哪一步保存下来\n",
    "    global_step = tf.Variable(\n",
    "        tf.zeros([], tf.int64), name = 'global_step', trainable = 'False')\n",
    "    \n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope(\n",
    "        'embedding', initializer = embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embedding',\n",
    "            [vocab_size, hps.num_embedding_size],\n",
    "            tf.float32)\n",
    "        # [1, 10, 7] -> [embeddings[1], embeddings[10], embeddings[7]]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    \n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope(\n",
    "        'lstm_nn', initializer = lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "                hps.num_lstm_nodes[i],\n",
    "                state_is_tuple = True)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell,\n",
    "                output_keep_prob = keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        \n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        # rnn_outputs: [batch_size, num_timesteps, lstm_outputs[-1]]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(\n",
    "            cell, embed_inputs, initial_state = initial_state)\n",
    "        last = rnn_outputs[:, -1, :]\n",
    "        \n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor = 1.0)\n",
    "    with tf.variable_scope(\n",
    "        'fc', initializer = fc_init):\n",
    "        fc1 = tf.layers.dense(last, \n",
    "                              hps.num_fc_nodes, \n",
    "                              activation = tf.nn.relu, \n",
    "                              name = 'fc1')\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        logits = tf.layers.dense(fc1_dropout,\n",
    "                                 num_classes, \n",
    "                                 name = 'fc2')\n",
    "    \n",
    "    with tf.name_scope('metrics'):\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits = logits, labels = outputs)\n",
    "        loss = tf.reduce_mean(softmax_loss)\n",
    "        # [0, 1, 5, 4, 2] -> argmax: 2\n",
    "        y_pred = tf.argmax(tf.nn.softmax(logits), \n",
    "                           1,\n",
    "                           output_type = tf.int32)\n",
    "        correct_pred = tf.equal(outputs, y_pred)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "    with tf.name_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info(\"variable name: %s\", var.name)\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars), global_step = global_step)\n",
    "        \n",
    "    return ((inputs, outputs, keep_prob), \n",
    "            (loss, accuracy),\n",
    "            (train_op, global_step))\n",
    "\n",
    "placeholders, metrics, others = create_model(\n",
    "    hps, vocab_size, num_classes)\n",
    "\n",
    "inputs, outputs, keep_prob = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:   100, loss: 2.110, accuracy: 0.19000\n",
      "INFO:tensorflow:Step:   200, loss: 1.778, accuracy: 0.28000\n",
      "INFO:tensorflow:Step:   300, loss: 1.797, accuracy: 0.30000\n",
      "INFO:tensorflow:Step:   400, loss: 1.488, accuracy: 0.37000\n",
      "INFO:tensorflow:Step:   500, loss: 1.416, accuracy: 0.43000\n",
      "INFO:tensorflow:Step:   600, loss: 1.225, accuracy: 0.48000\n",
      "INFO:tensorflow:Step:   700, loss: 1.439, accuracy: 0.52000\n",
      "INFO:tensorflow:Step:   800, loss: 1.443, accuracy: 0.43000\n",
      "INFO:tensorflow:Step:   900, loss: 1.155, accuracy: 0.58000\n",
      "INFO:tensorflow:Step:  1000, loss: 1.053, accuracy: 0.55000\n",
      "INFO:tensorflow:Step:  1100, loss: 1.034, accuracy: 0.61000\n",
      "INFO:tensorflow:Step:  1200, loss: 0.817, accuracy: 0.66000\n",
      "INFO:tensorflow:Step:  1300, loss: 1.015, accuracy: 0.61000\n",
      "INFO:tensorflow:Step:  1400, loss: 0.895, accuracy: 0.63000\n",
      "INFO:tensorflow:Step:  1500, loss: 0.810, accuracy: 0.71000\n",
      "INFO:tensorflow:Step:  1600, loss: 0.752, accuracy: 0.73000\n",
      "INFO:tensorflow:Step:  1700, loss: 0.745, accuracy: 0.71000\n",
      "INFO:tensorflow:Step:  1800, loss: 0.707, accuracy: 0.75000\n",
      "INFO:tensorflow:Step:  1900, loss: 0.735, accuracy: 0.78000\n",
      "INFO:tensorflow:Step:  2000, loss: 0.653, accuracy: 0.75000\n",
      "INFO:tensorflow:Step:  2100, loss: 0.658, accuracy: 0.68000\n",
      "INFO:tensorflow:Step:  2200, loss: 0.578, accuracy: 0.77000\n",
      "INFO:tensorflow:Step:  2300, loss: 0.869, accuracy: 0.67000\n",
      "INFO:tensorflow:Step:  2400, loss: 0.568, accuracy: 0.83000\n",
      "INFO:tensorflow:Step:  2500, loss: 0.521, accuracy: 0.73000\n",
      "INFO:tensorflow:Step:  2600, loss: 0.631, accuracy: 0.75000\n",
      "INFO:tensorflow:Step:  2700, loss: 0.666, accuracy: 0.80000\n",
      "INFO:tensorflow:Step:  2800, loss: 0.288, accuracy: 0.89000\n",
      "INFO:tensorflow:Step:  2900, loss: 0.502, accuracy: 0.79000\n",
      "INFO:tensorflow:Step:  3000, loss: 0.355, accuracy: 0.88000\n",
      "INFO:tensorflow:Step:  3100, loss: 0.276, accuracy: 0.90000\n",
      "INFO:tensorflow:Step:  3200, loss: 0.478, accuracy: 0.83000\n",
      "INFO:tensorflow:Step:  3300, loss: 0.386, accuracy: 0.88000\n",
      "INFO:tensorflow:Step:  3400, loss: 0.218, accuracy: 0.94000\n",
      "INFO:tensorflow:Step:  3500, loss: 0.277, accuracy: 0.92000\n",
      "INFO:tensorflow:Step:  3600, loss: 0.298, accuracy: 0.89000\n",
      "INFO:tensorflow:Step:  3700, loss: 0.245, accuracy: 0.95000\n",
      "INFO:tensorflow:Step:  3800, loss: 0.375, accuracy: 0.90000\n",
      "INFO:tensorflow:Step:  3900, loss: 0.198, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  4000, loss: 0.285, accuracy: 0.94000\n",
      "INFO:tensorflow:Step:  4100, loss: 0.248, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  4200, loss: 0.100, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  4300, loss: 0.093, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  4400, loss: 0.114, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  4500, loss: 0.173, accuracy: 0.95000\n",
      "INFO:tensorflow:Step:  4600, loss: 0.166, accuracy: 0.94000\n",
      "INFO:tensorflow:Step:  4700, loss: 0.083, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  4800, loss: 0.195, accuracy: 0.96000\n",
      "INFO:tensorflow:Step:  4900, loss: 0.295, accuracy: 0.92000\n",
      "INFO:tensorflow:Step:  5000, loss: 0.134, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  5100, loss: 0.083, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  5200, loss: 0.214, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  5300, loss: 0.148, accuracy: 0.96000\n",
      "INFO:tensorflow:Step:  5400, loss: 0.282, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  5500, loss: 0.053, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  5600, loss: 0.044, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  5700, loss: 0.215, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  5800, loss: 0.034, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  5900, loss: 0.089, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  6000, loss: 0.082, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  6100, loss: 0.183, accuracy: 0.96000\n",
      "INFO:tensorflow:Step:  6200, loss: 0.028, accuracy: 1.00000\n",
      "INFO:tensorflow:Step:  6300, loss: 0.305, accuracy: 0.94000\n",
      "INFO:tensorflow:Step:  6400, loss: 0.175, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  6500, loss: 0.084, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  6600, loss: 0.036, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  6700, loss: 0.206, accuracy: 0.95000\n",
      "INFO:tensorflow:Step:  6800, loss: 0.006, accuracy: 1.00000\n",
      "INFO:tensorflow:Step:  6900, loss: 0.085, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  7000, loss: 0.023, accuracy: 1.00000\n",
      "INFO:tensorflow:Step:  7100, loss: 0.148, accuracy: 0.96000\n",
      "INFO:tensorflow:Step:  7200, loss: 0.058, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  7300, loss: 0.050, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  7400, loss: 0.036, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  7500, loss: 0.083, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  7600, loss: 0.027, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  7700, loss: 0.041, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  7800, loss: 0.024, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  7900, loss: 0.071, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  8000, loss: 0.054, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  8100, loss: 0.051, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  8200, loss: 0.136, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  8300, loss: 0.014, accuracy: 1.00000\n",
      "INFO:tensorflow:Step:  8400, loss: 0.042, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  8500, loss: 0.006, accuracy: 1.00000\n",
      "INFO:tensorflow:Step:  8600, loss: 0.057, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  8700, loss: 0.042, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  8800, loss: 0.014, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  8900, loss: 0.078, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  9000, loss: 0.084, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  9100, loss: 0.007, accuracy: 1.00000\n",
      "INFO:tensorflow:Step:  9200, loss: 0.057, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  9300, loss: 0.020, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  9400, loss: 0.100, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  9500, loss: 0.126, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  9600, loss: 0.045, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  9700, loss: 0.001, accuracy: 1.00000\n",
      "INFO:tensorflow:Step:  9800, loss: 0.045, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  9900, loss: 0.004, accuracy: 1.00000\n",
      "INFO:tensorflow:Step: 10000, loss: 0.056, accuracy: 0.98000\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.8\n",
    "test_keep_prob_value = 1.0\n",
    "\n",
    "num_train_steps = 10000\n",
    "\n",
    "# Train: 99.7\n",
    "# Valid: 92.7\n",
    "# Test: 93.2\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs, batch_labels = train_dataset.next_batch(\n",
    "            hps.batch_size)\n",
    "        outputs_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                           feed_dict = {\n",
    "                               inputs: batch_inputs,\n",
    "                               outputs: batch_labels,\n",
    "                               keep_prob: train_keep_prob_value\n",
    "                           })\n",
    "        loss_val, accuarcy_val, _, global_step_val = outputs_val\n",
    "        if global_step_val % 100 == 0:\n",
    "            tf.logging.info(\"Step: %5d, loss: %3.3f, accuracy: %3.5f\"\n",
    "                           % (global_step_val, loss_val, accuarcy_val))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
